{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification, ViTConfig, TrainingArguments, Trainer\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from datasets import Dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image and Labels Loading and Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Data from CSV\n",
    "csv_file = \"15_clean_MovieGenre.csv\"  # Path to your CSV file\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# for 23\n",
    "# binarized_columns = ['Action', 'Adventure', 'Animation', 'Biography', 'Comedy', 'Crime', 'Documentary', \n",
    "#                      'Drama', 'Family', 'Fantasy', 'Film-Noir', 'History', 'Horror', 'Music', 'Musical', \n",
    "#                      'Mystery', 'Romance', 'Sci-Fi', 'Short', 'Sport', 'Thriller', 'War', 'Western']\n",
    "\n",
    "#for 15\n",
    "binarized_columns = ['Action', 'Adventure', 'Animation', 'Biography', 'Comedy', 'Crime', 'Documentary', \n",
    "                     'Drama', 'Family', 'Fantasy', 'Horror', 'Romance', 'Thriller']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, images, labels, processor):\n",
    "        self.images = images\n",
    "        self.labels = labels.astype(np.float32)\n",
    "        self.processor = processor  # Store the processor as an attribute\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Process image using the provided processor\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "        processed_image = inputs.pixel_values.squeeze(0)  # Remove batch dimension\n",
    "\n",
    "        return {\"pixel_values\": processed_image, \"labels\": label}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = \"sample_images\" #sample_images for developing, downloaded_images for final\n",
    "images = []\n",
    "labels = []\n",
    "batch_size = 128  \n",
    "\n",
    "# Load images and labels in batches\n",
    "num_batches = len(df) // batch_size + 1\n",
    "\n",
    "for batch_num in tqdm(range(num_batches)):\n",
    "    start_idx = batch_num * batch_size\n",
    "    end_idx = min((batch_num + 1) * batch_size, len(df))\n",
    "    \n",
    "    batch_df = df.iloc[start_idx:end_idx]\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "    \n",
    "    for index, row in batch_df.iterrows():\n",
    "        filename = str(row.iloc[0]) + \".jpg\"  # filenames match the imdbIDs\n",
    "        label = [int(row[column]) for column in binarized_columns]  # Extract binarized labels for each genre\n",
    "\n",
    "        image_path = os.path.join(image_folder, filename)\n",
    "        try:\n",
    "            image = Image.open(image_path)  # Open image using PIL\n",
    "            image = image.convert(\"RGB\")  # Convert image to RGB mode if necessary\n",
    "            image_array = np.array(image)  # Convert PIL Image to numpy array\n",
    "            batch_images.append(image_array)\n",
    "            batch_labels.append(label)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    # Concatenate the batches of images and labels\n",
    "    if batch_images:\n",
    "        images.append(np.array(batch_images))\n",
    "        labels.append(np.array(batch_labels))\n",
    "\n",
    "# Concatenate all batches into single numpy arrays\n",
    "if images:\n",
    "    images = np.concatenate(images, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Posters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shapes of the loaded data\n",
    "print(\"Images shape:\", images.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "\n",
    "# Display sample images\n",
    "plt.figure(figsize=(12, 12))\n",
    "for i in range(9):\n",
    "\tax = plt.subplot(3, 3, i + 1)\n",
    "\tplt.imshow(images[i])\n",
    "\tplt.axis('off')\n",
    "\tplt.title([column for column, label in zip(binarized_columns, labels[i]) if label == 1], fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Print the shapes of the split datasets\n",
    "print(\"Training images shape:\", X_train.shape)\n",
    "print(\"Training labels shape:\", y_train.shape)\n",
    "\n",
    "print(\"Validation images shape:\", X_val.shape)\n",
    "print(\"Validation labels shape:\", y_val.shape)\n",
    "\n",
    "print(\"Testing images shape:\", X_test.shape)\n",
    "print(\"Testing labels shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display Distribution\n",
    "train_label_distribution = np.sum(y_train, axis=0)\n",
    "val_label_distribution = np.sum(y_val, axis=0)\n",
    "test_label_distribution = np.sum(y_test, axis=0)\n",
    "\n",
    "combined_data = {\n",
    "    \"Label\": binarized_columns,\n",
    "    \"Train Distribution\": train_label_distribution,\n",
    "    \"Validation Distribution\": val_label_distribution,\n",
    "    \"Test Distribution\": test_label_distribution\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(combined_data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to the range [0, 1] and convert to float32\n",
    "X_train = (X_train / 255.0)\n",
    "X_val = (X_val / 255.0)\n",
    "X_test = (X_test / 255.0)\n",
    "\n",
    "X_train = np.transpose(X_train, (0, 3, 1, 2))\n",
    "X_val = np.transpose(X_val, (0, 3, 1, 2))\n",
    "X_test = np.transpose(X_test, (0, 3, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure labels are int64\n",
    "y_train = y_train.astype(np.int64)\n",
    "y_val = y_val.astype(np.int64)\n",
    "y_test = y_test.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shard_dataset(X, y, shard_size=10000):\n",
    "    for i in range(0, len(X), shard_size):\n",
    "        yield {'pixel_values': X[i:i + shard_size], 'labels': y[i:i + shard_size]}\n",
    "\n",
    "# Shard the training dataset and convert all to datasets\n",
    "train_dataset = concatenate_datasets([Dataset.from_dict(shard) for shard in shard_dataset(X_train, y_train)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = {\"pixel_values\": X_val, \"labels\": y_val}\n",
    "test_data = {\"pixel_values\": X_test, \"labels\": y_test}\n",
    "\n",
    "val_dataset = Dataset.from_dict(val_data)\n",
    "test_dataset = Dataset.from_dict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the default ViT model configuration\n",
    "vit_config = ViTConfig.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "\n",
    "# Update the image size in the configuration\n",
    "vit_config = ViTConfig(\n",
    "    image_size=(268, 182),\n",
    "    problem_type=\"multi_label_classification\",\n",
    "\tnum_labels=len(binarized_columns),\n",
    "    is_classifier=True,\n",
    "    classifier_activation=\"sigmoid\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Specification\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k', config=vit_config, ignore_mismatched_sizes=True)\n",
    "\n",
    "#utilize cuda if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "model = model.to(device)\n",
    "\n",
    "# # Utilize MPS backend if available (specific to Apple Silicon)\n",
    "# device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "# print(\"Using device:\", device)\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    logits = pred.predictions\n",
    "    labels = pred.label_ids\n",
    "    \n",
    "    # Apply sigmoid to get probabilities\n",
    "    probabilities = torch.sigmoid(torch.tensor(logits)).numpy()\n",
    "    \n",
    "    # Define a threshold to get binary predictions\n",
    "    threshold = 0.5\n",
    "    predictions = (probabilities > threshold).astype(int)\n",
    "    \n",
    "    f1_weighted = f1_score(labels, predictions, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'f1_weighted': f1_weighted\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights based on the class distribution in the training data\n",
    "class_counts = np.sum(y_train, axis=0)\n",
    "total_samples = len(y_train)\n",
    "class_weights = torch.tensor(total_samples / (len(binarized_columns) * class_counts), dtype=torch.float32)\n",
    "\n",
    "# Custom Trainer\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\").float()  # Ensure labels are float\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Define the loss function with class weights\n",
    "        loss_fct = BCEWithLogitsLoss(weight=class_weights.to(logits.device))\n",
    "        loss = loss_fct(logits, labels)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Macbook\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='output',\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,  \n",
    "    per_device_eval_batch_size=16,   \n",
    "    num_train_epochs=1,             \n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,               \n",
    "    save_steps=200,  \n",
    "    eval_steps=10,               \n",
    "    evaluation_strategy=\"steps\",  \n",
    "    # gradient_accumulation_steps=2,  \n",
    "    learning_rate=2e-5,            \n",
    "    # weight_decay=0.01,             \n",
    "    # save_total_limit=2,            \n",
    "    lr_scheduler_type='cosine_with_restarts',  # Example of using a cosine annealing LR scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_args = TrainingArguments(\n",
    "    output_dir=\"results\",\n",
    "    do_train=False,\n",
    "    do_eval=False,\n",
    "    do_predict=True,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=trainer.model,\n",
    "    args=test_args\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.map(lambda example: {'labels': example['labels'].astype(int)})\n",
    "predictions = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_np = np.array(predictions.predictions > threshold, dtype=int)\n",
    "\n",
    "# Initialize lists to store F1 scores and accuracies for each label\n",
    "f1_scores = []\n",
    "accuracies = []\n",
    "\n",
    "# Iterate over each label\n",
    "for i, label in enumerate(binarized_columns):\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_test_np[:, i], predicted_labels_np[:, i], average='binary')\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test[:, i], predicted_labels_np[:, i])\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Calculate overall F1 score using micro and weighted averages\n",
    "f1_weighted = f1_score(y_test_np, predicted_labels_np, average='weighted')\n",
    "\n",
    "# Create DataFrame to display label-wise metrics\n",
    "data = {\n",
    "    \"Label\": binarized_columns,\n",
    "    \"F1 Score\": f1_scores,\n",
    "    \"Accuracy\": accuracies\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "\n",
    "# Print the overall F1 scores\n",
    "print(\"\\nWeighted-averaged F1 score:\", f1_weighted)\n",
    "\n",
    "# Compute average F1 score per label\n",
    "avg_f1_score = np.mean(f1_scores)\n",
    "print(\"Average F1 score (per label):\", avg_f1_score)\n",
    "\n",
    "# Compute average accuracy\n",
    "average_accuracy = sum(accuracies) / len(accuracies)\n",
    "print(\"Average accuracy:\", average_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(X_test, y_test, predicted_probabilities, binarized_columns, threshold=0.5):\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "\n",
    "    for idx, ax in enumerate(axes.flat):\n",
    "        if idx < len(X_test):\n",
    "            poster = X_test[idx].transpose(1, 2, 0)  # Transpose back to (height, width, channels)\n",
    "            true_labels = [binarized_columns[i] for i, label in enumerate(y_test[idx]) if label == 1]\n",
    "            predicted_genre_probabilities = predicted_probabilities[idx]\n",
    "\n",
    "            # Sort predicted probabilities and select labels based on the number of true labels\n",
    "            num_true_labels = len(true_labels)\n",
    "            top_predicted_idx = np.argsort(predicted_genre_probabilities)[::-1][:num_true_labels]\n",
    "            predicted_genre_labels = [binarized_columns[i] for i in top_predicted_idx]\n",
    "            top_predicted_probabilities = predicted_genre_probabilities[top_predicted_idx]\n",
    "\n",
    "            ax.imshow(poster)\n",
    "            ax.set_title(f\"True Genres: {true_labels}\\nPredicted Genres: {predicted_genre_labels}\\nProbabilities: {top_predicted_probabilities}\", fontsize=10)\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.axis('off')  # Hide empty subplots\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "display(X_test, y_test, predicted_probabilities, binarized_columns)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
